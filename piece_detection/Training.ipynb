{"cells":[{"cell_type":"code","execution_count":null,"id":"2fefae6c","metadata":{"id":"2fefae6c"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"id":"70052744-282c-47bc-a057-cf84616777ea","metadata":{"id":"70052744-282c-47bc-a057-cf84616777ea"},"outputs":[],"source":["cd  \"/content/drive/MyDrive/final_proj/task2\"\n"]},{"cell_type":"code","execution_count":null,"id":"Te2FH1ITBg_6","metadata":{"id":"Te2FH1ITBg_6"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import json\n"]},{"cell_type":"code","execution_count":null,"id":"c2ae4b80","metadata":{"id":"c2ae4b80"},"outputs":[],"source":["label_map = {'r': 1, 'n': 2, 'b': 3, 'k': 4, 'q': 5, 'p': 6, 'R': 7, 'N': 8, 'B': 9, 'K': 10, 'Q': 11, 'P': 12}\n"]},{"cell_type":"code","execution_count":null,"id":"516e3327","metadata":{"id":"516e3327"},"outputs":[],"source":["class ChessDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.image_list = sorted([file for file in os.listdir(os.path.join(root_dir)) if file.endswith('.png')])\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir, self.image_list[idx])\n","        json_name = os.path.splitext(img_name)[0] + \".json\"\n","\n","        image = Image.open(img_name).convert(\"RGB\")\n","        with open(json_name) as f:\n","            annotation = json.load(f)\n","\n","        # extract labels and boxes of chess pieces\n","        pieces_info = annotation['pieces']\n","        labels = [label_map[piece['piece']] for piece in pieces_info]\n","        boxes = [piece['box'] for piece in pieces_info]\n","\n","        # convert box coordinates to (xmin, ymin, xmax, ymax) format\n","        boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in boxes]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # return image, targets (labels and boxes)\n","        targets = {\n","            'boxes': torch.tensor(boxes, dtype=torch.float32),\n","            'labels': torch.tensor(labels)\n","        }\n","\n","        return image, targets,img_name\n"]},{"cell_type":"code","execution_count":null,"id":"11495c3e","metadata":{"id":"11495c3e"},"outputs":[],"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n"]},{"cell_type":"code","execution_count":null,"id":"c7672ef3","metadata":{"id":"c7672ef3"},"outputs":[],"source":["import torch\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n"]},{"cell_type":"code","execution_count":null,"id":"a1de82b5","metadata":{"id":"a1de82b5"},"outputs":[],"source":["def calculate_iou(box1, box2):\n","    x_left = max(box1[0], box2[0])\n","    y_top = max(box1[1], box2[1])\n","    x_right = min(box1[2], box2[2])\n","    y_bottom = min(box1[3], box2[3])\n","\n","    # calculate intersection area\n","    if x_right < x_left or y_bottom < y_top:\n","        return 0.0\n","    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n","\n","    # calculate areas of individual boxes\n","    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","\n","    # calculate union area\n","    union_area = area_box1 + area_box2 - intersection_area\n","\n","    # calculate IoU\n","    iou = intersection_area / union_area\n","    return iou\n"]},{"cell_type":"code","execution_count":null,"id":"d21593ec","metadata":{"id":"d21593ec"},"outputs":[],"source":["def find_golden_match(gts, pred, pred_idx, threshold=0.5):\n","\n","\n","    golden_match_idx = -1\n","    best_iou = -1\n","\n","    for idx, gt in enumerate(gts):\n","        iou = calculate_iou(gt, pred)\n","\n","        if iou >= threshold and iou > best_iou:\n","            best_iou = iou\n","            golden_match_idx = idx\n","\n","    return golden_match_idx\n"]},{"cell_type":"code","execution_count":null,"id":"5be0f411","metadata":{"id":"5be0f411"},"outputs":[],"source":["def calculate_metrics(gts, preds, threshold = 0.5):\n","\n","    n = len(preds)\n","    tp = 0\n","    fp = 0\n","    fns=[1 for i in range(len(gts))]\n","    for pred_idx in range(n):\n","\n","        golden_match_gt_idx = find_golden_match(gts, preds[pred_idx], pred_idx,\n","                                            threshold=threshold)\n","\n","        if golden_match_gt_idx >= 0:\n","            tp += 1\n","            fns[golden_match_gt_idx]=0\n","        else:\n","            fp += 1\n","\n","    fn = sum(fns)\n","    p=tp / (tp + fp + fn)\n","    r=tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","    f1=2*p*r/(p+r) if (p+r)>0 else 0.0\n","    return p,r,f1\n"]},{"cell_type":"code","execution_count":null,"id":"dcb684c9","metadata":{"id":"dcb684c9"},"outputs":[],"source":["def evaluate_val(model, val_loader, device):\n","    model.eval()\n","    running_val_loss = 0.0\n","    count=0\n","    validation_image_precisions=[]\n","    validation_image_recalls=[]\n","    validation_image_f1s=[]\n","    with torch.no_grad():\n","        for _, (images, targets,image_names)  in enumerate(val_loader):\n","\n","            images = list(image.to(device) for image in images)\n","            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","            outputs = model(images)\n","        for i, image in enumerate(images):\n","            boxes = outputs[i]['boxes'].data.cpu().numpy()\n","            scores = outputs[i]['scores'].data.cpu().numpy()\n","            gt_boxes = targets[i]['boxes'].cpu().numpy()\n","            preds_sorted_idx = np.argsort(scores)[::-1]\n","            preds_sorted = boxes[preds_sorted_idx]\n","\n","            image_precision,image_recall,image_f1 = calculate_metrics(preds_sorted,\n","                                        gt_boxes,\n","                                        threshold = 0.5\n","                                        )\n","            validation_image_precisions.append(image_precision)\n","            validation_image_recalls.append(image_recall)\n","            validation_image_f1s.append(image_f1)\n","        valid_prec = np.mean(validation_image_precisions)\n","        valid_recall = np.mean(validation_image_recalls)\n","        valid_f1=np.mean(validation_image_f1s)\n","        return valid_prec,valid_recall,valid_f1\n"]},{"cell_type":"code","execution_count":null,"id":"d294f74d","metadata":{"id":"d294f74d"},"outputs":[],"source":["def train(model,device,epochs,optimizer,train_loader,val_loader):\n","\n","    print_every = 50  # print loss every 100 steps\n","    best_val_f1= -float('inf')\n","    for epoch in range(epochs):\n","\n","        running_train_loss = 0.0\n","\n","        for step, (images, targets,image_names)  in enumerate(train_loader):\n","            model.train()\n","            images = list(image.to(device) for image in images)\n","\n","            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","            loss_dict = model(images, targets)\n","\n","\n","            losses = sum(loss for loss in loss_dict.values())\n","\n","            optimizer.zero_grad()\n","            losses.backward()\n","            optimizer.step()\n","\n","            running_train_loss += losses.item()\n","\n","            # print training loss every print_every steps\n","            if (step + 1) % print_every == 0 or step+1==len(train_loader):\n","                epoch_train_loss = running_train_loss / print_every\n","\n","                running_train_loss = 0.0\n","\n","\n","                val_prec,val_recall,val_f1 = evaluate_val(model, val_loader,device)\n","                print(f\"Epoch [{epoch+1}/{epochs}] - Step : [{step+1}/{len(train_loader)}] - Train Loss: {epoch_train_loss:.4f} - Val Precision: {val_prec:.4f} - Val Recall: {val_recall:.4f}- Val F1: {val_f1:.4f}\")\n","\n","                # save the best model based on validation loss\n","                if  val_f1> best_val_f1:\n","                    best_val_f1 = val_f1\n","                    torch.save(model.state_dict(), 'models/best_model_ep{}_s{}_f1{}.pth'.format(epoch+1,step+1,round(val_f1,4)))\n"]},{"cell_type":"code","execution_count":null,"id":"cf50c1b3","metadata":{"id":"cf50c1b3"},"outputs":[],"source":["batch_size=18\n","epochs=10\n","lr=3e-4\n","\n","train_directory = \"train/\"\n","val_directory = \"val/\"\n","\n","transform = transforms.Compose([transforms.ToTensor()])\n","train_dataset = ChessDataset(train_directory, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","val_dataset = ChessDataset(val_directory, transform=transform)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"]},{"cell_type":"code","execution_count":null,"id":"ef7a0c00","metadata":{"id":"ef7a0c00","outputId":"b60f5f23-9365-4a9d-b2d2-12d0d9879e17"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tianzhengg/miniconda3/envs/cp/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/home/tianzhengg/miniconda3/envs/cp/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/10] - Step : [50/245] - Train Loss: 1.0387 - Val Precision: 0.2850 - Val Recall: 0.2850- Val F1: 0.2850\n","Epoch [1/10] - Step : [100/245] - Train Loss: 0.3443 - Val Precision: 0.3877 - Val Recall: 0.3877- Val F1: 0.3877\n","Epoch [1/10] - Step : [150/245] - Train Loss: 0.2189 - Val Precision: 0.4828 - Val Recall: 0.4828- Val F1: 0.4828\n","Epoch [1/10] - Step : [200/245] - Train Loss: 0.1661 - Val Precision: 0.5747 - Val Recall: 0.5747- Val F1: 0.5747\n","Epoch [1/10] - Step : [245/245] - Train Loss: 0.1315 - Val Precision: 0.6292 - Val Recall: 0.6292- Val F1: 0.6292\n","Epoch [2/10] - Step : [50/245] - Train Loss: 0.1299 - Val Precision: 0.7969 - Val Recall: 0.7969- Val F1: 0.7969\n","Epoch [2/10] - Step : [100/245] - Train Loss: 0.1198 - Val Precision: 0.8409 - Val Recall: 0.8409- Val F1: 0.8409\n","Epoch [2/10] - Step : [150/245] - Train Loss: 0.1071 - Val Precision: 0.8385 - Val Recall: 0.8385- Val F1: 0.8385\n"]}],"source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","num_classes = 13  # 12 class + background\n","\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","train(model,device,epochs,optimizer, train_loader,val_loader)\n"]},{"cell_type":"code","execution_count":null,"id":"24b369fa","metadata":{"id":"24b369fa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e8ff2264","metadata":{"id":"e8ff2264"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"6871e69f","metadata":{"id":"6871e69f"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"197788f3","metadata":{"id":"197788f3"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"cp","language":"python","name":"cp"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
